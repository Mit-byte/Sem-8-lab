{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c5e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d790d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample sentence\n",
    "sentence = \"This is a sample sentence for tokenization, stemming, and lemmatization using NLTK library in Python!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a3670",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9460d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization:  ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization,', 'stemming,', 'and', 'lemmatization', 'using', 'NLTK', 'library', 'in', 'Python!']\n"
     ]
    }
   ],
   "source": [
    "# Whitespace tokenization\n",
    "whitespace_tokens = sentence.split()\n",
    "print(\"Whitespace Tokenization: \", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cea55",
   "metadata": {},
   "source": [
    "<b>Punctuation-based tokenization:</b>\n",
    "Punctuation-based tokenization is slightly more advanced than whitespace-based tokenization since it splits on whitespace and punctuations and also retains the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f37faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based Tokenization:  ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'using', 'NLTK', 'library', 'in', 'Python', '!']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation-based tokenization\n",
    "punctuation_tokens = word_tokenize(sentence)\n",
    "print(\"Punctuation-based Tokenization: \", punctuation_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d34cdf",
   "metadata": {},
   "source": [
    "<b>Treebank Tonkenization:</b>\n",
    "    This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I'm, don't) and hyphenated words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad88cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization:  ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'using', 'NLTK', 'library', 'in', 'Python', '!']\n"
     ]
    }
   ],
   "source": [
    "# Treebank tokenization\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = treebank_tokenizer.tokenize(sentence)\n",
    "print(\"Treebank Tokenization: \", treebank_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86de0dc",
   "metadata": {},
   "source": [
    "<b>Tweet Tokenization:</b>\n",
    "NLTK has this special method called TweetTokenizer() that helps to tokenize Tweet Corpus into relevant tokens. The advantage of using TweetTokenizer() compared to regular word_tokenize is that, when processing tweets, we often come across emojis, hashtags that need to be handled differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c39a4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenization:  ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', 'using', 'NLTK', 'library', 'in', 'Python', '!']\n"
     ]
    }
   ],
   "source": [
    "# Tweet tokenization\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sentence)\n",
    "print(\"Tweet Tokenization: \", tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232ce24",
   "metadata": {},
   "source": [
    "<b>Mulit-word expression Tokenization:</b>\n",
    "The multi-word expression tokenizer is a rule-based, “add-on” tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02177146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MWE Tokenization:  ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization,', 'stemming,', 'and', 'lemmatization', 'using', 'NLTK_library', 'in', 'Python!']\n"
     ]
    }
   ],
   "source": [
    "# Multi-word expression tokenization\n",
    "mwe_tokenizer = MWETokenizer([('NLTK', 'library')]) # In output NLTK and Library is a combined single token\n",
    "mwe_tokens = mwe_tokenizer.tokenize(sentence.split())\n",
    "print(\"MWE Tokenization: \", mwe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b7a57",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a829d2",
   "metadata": {},
   "source": [
    "<b>Porter Stemmer:</b>\n",
    "It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. This stemmer is known for its speed and simplicity.\n",
    "\n",
    "Example: EED -> EE means “if the word has at least one vowel and consonant plus EED ending, change the ending to EE” as\n",
    "‘<b>agreed</b>’ becomes ‘<b>agree</b>’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d9aca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Porter Stemmer:  ['thi', 'is', 'a', 'sampl', 'sentenc', 'for', 'token', ',', 'stem', ',', 'and', 'lemmat', 'use', 'nltk', 'librari', 'in', 'python', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming using Porter Stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens_porter = [porter_stemmer.stem(token) for token in punctuation_tokens]\n",
    "print(\"Stemming using Porter Stemmer: \", stemmed_tokens_porter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14db15",
   "metadata": {},
   "source": [
    "<b>Snowball Stemmer:</b>\n",
    "\n",
    "<i> multi-lingual stemmer\n",
    "\n",
    "<i> more aggressive than Porter Stemmer and is also referred to as Porter2 Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9fb824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming using Snowball Stemmer:  ['this', 'is', 'a', 'sampl', 'sentenc', 'for', 'token', ',', 'stem', ',', 'and', 'lemmat', 'use', 'nltk', 'librari', 'in', 'python', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming using Snowball Stemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "stemmed_tokens_snowball = [snowball_stemmer.stem(token) for token in punctuation_tokens]\n",
    "print(\"Stemming using Snowball Stemmer: \", stemmed_tokens_snowball)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc45a03",
   "metadata": {},
   "source": [
    "<b>Lemmatization using WordNetLemmatizer:</b><br>\n",
    "Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships between its words.\n",
    "\n",
    "Wordnet links words into semantic relations. ( eg. synonyms )<br>\n",
    "It groups synonyms in the form of synsets.<br>\n",
    "synsets : a group of data elements that are semantically equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3836713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization using WordNetLemmatizer:  ['This', 'be', 'a', 'sample', 'sentence', 'for', 'tokenization', ',', 'stem', ',', 'and', 'lemmatization', 'use', 'NLTK', 'library', 'in', 'Python', '!']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "for token in punctuation_tokens:\n",
    "    pos_tag = nltk.pos_tag([token])[0][1][0].lower()\n",
    "    if pos_tag == 'j':\n",
    "        pos_tag = 'a'\n",
    "    elif pos_tag in ['v', 'n']:\n",
    "        pos_tag = pos_tag\n",
    "    else:\n",
    "        pos_tag = 'n'\n",
    "    lemma = wordnet_lemmatizer.lemmatize(token, pos_tag)\n",
    "    lemmatized_tokens.append(lemma)\n",
    "print(\"Lemmatization using WordNetLemmatizer: \", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0f85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
